---
title: "Chapter 5 - Spending our Data"
format: 
    html:
        embed-resources: true
---


Chapter starts out with introducing the idea of a "data budget". The authors argue there is a careful
balance between parameter estimation, model selection and tuning, and performance assessment.  With the
finite data available, folks need to be careful 'spending' data for each of these tasks.  If data is 'reused'
folks risk compounding biases and methodological errors, thus decreasing empiral validation.

## Common methods for splitting data

- split data into two distinct sets: training and testing.
    - training is usually majority of the data
        - a sandbox for model building and fitting etc...
    - testing is the rest
        - test is the final yard stick
            - should only be looked at **once** otherwise it's a part of
            the modeling process.

```{r setup}
here::i_am(fs::path("tidy-modeling-with-r", "chapter-07.qmd"))
library(here)
source(here("tidy-modeling-with-r", "setup.R"))
```


## simple random sample
going to use the `rsample` package to split the data up into the two groups.


```{r}
# set seed to match book
set.seed(501)

ames_split <- rsample::initial_split(ames, prop = 0.80)
ames_split
# ^ only contains partitioning information

# apply more funs to get the data
ames_train <- rsample::training(ames_split)
ames_test <- rsample::testing(ames_split)

dim(ames_train)
```


authors go on to talk about how simple random sampling is good in many cases, but not all.
if the prevalence of categoricals is out of balance, there may be an inbalance in the test/train datasets.
use stratified in that case.

Authors state that you can use artificical quartiles and then conduct stratified sampling x 4.


```{r}
quartiles <- summary(ames$Sale_Price)

ames |> 
    ggplot() +
    geom_density(aes(Sale_Price)) +
    geom_vline(xintercept = quartiles[[2]]) +
    geom_vline(xintercept = quartiles[[3]]) +
    geom_vline(xintercept = quartiles[[5]]) 

# def an easier way to do that.... but I forget :D
```


## stratified sampling


```{r}
set.seed(502)
ames_split <- rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- rsample::training(ames_split)
ames_test <- rsample::testing(ames_split)

dim(ames_train)
```

looks good. only  a single column can be used to stratify per the authors. they also note there is very 
little downside to using stratified sampling.

what about when we don't want random sampling?>

with time series it is common to use the most recent data as the test set.
so `rsample` has `initial_time_split()`. but prop, instead, says what proportion of the first bits of data
should be training - assuming data is pre sorted.


## what about validation?

if someone wanted a validation set they may do something like this:

```{r}
# 60% train, 20% validate 20% test

ames_val_split <- rsample::initial_validation_split(ames, prop = c(0.6, 0.2))
ames_val_split

ames_train <- training(ames_val_split)
ames_test <- testing(ames_val_split)
ames_val <- validation(ames_val_split)

```

## other considerations

watch out for data leakage!


```{r reset-for-next-chapter}

if (interactive()) {
    ames <- read_ames()

    set.seed(502)
    ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
    ames_train <- training(ames_split)
    ames_test  <-  testing(ames_split) 
}

```